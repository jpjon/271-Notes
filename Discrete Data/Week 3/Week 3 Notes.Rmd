```{r}
library(car)
placekick <- read.table( file = "./Placekick.csv" , header = TRUE , sep = ",")
```

# Interactions and transformations for explanatory variables

### Interactions

Interactions between explanatory variables are needed when the effect of one explanatory variable on the probability of success depends on the value for a second explanatory variable.

Ex: logit($\large \pi$) = $\large \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2$

We change the **formula** argument in **glm()** to do so. Equivalent ways to code the model are:

1.  formula = y \~ x1 + x2 + x1:x2 #colon denotes interaction between x1 and x2
2.  formula = y \~ x1\*x2 \# asterisk automatically creates main effects (x1 and x2 by themselves) and interaction
3.  formula = y \~ (x1 + x2)\^2 \# creates combinations of all variables

### Transformations

An example is when the relationship between an explanatory variable and logit($\large \pi$) is not linear. To include a quadratic transformation, you use the identity function **I()** in the **formula** argument.

Ex: logit($\large \pi$) = $\large \beta_0 + \beta_1x_1 + \beta_2x_1^2$ is coded as **formula = y \~ x1 + I(x1 \^2)**

### Odds Ratio

Model example:

$$
\large logit(\pi) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2
$$

#### Interactions:

$$
\begin{align}
\large OR &= \large \frac{Odds_{x_2+c}}{Odds_{x2}} = \frac{exp(\beta_0 + \beta_1x_1 + \beta_2(x_2+c) + \beta_3x_1(x_2+c))}{exp(\beta_0 + \beta_1x_1 + \beta_2x_+ \beta_3x_1x_2)}\\
&=\large exp(c\beta_2 + c\beta_3x_1) = exp(c(\beta_2 + \beta_3x_1))
\end{align}
$$

Model example:

$$
\large logit(\pi) = \beta_0 + \beta_1x_1 + \beta_2x_1^2
$$

#### Transformations:

$$
\begin{align}
\large OR &= \large \frac{Odds_{x_1+c}}{Odds_{x1}} = \frac{exp(\beta_0 + \beta_1(x_1 + c) + \beta_2(x_1+c)^2)}{exp(\beta_0 + \beta_1x_1 + \beta_2x_1^2)}\\
&=\large exp(c\beta_1 + 2cx_1\beta_2 + c^2\beta_2) = exp(c(\beta_1 + c\beta_2(2x_1+c))
\end{align}
$$

#### Wald CI for Odds

CI for these odds ratios are complicated since confint() needs a linear combinations of regression parameters. It can't be used in this task.

$$
\large exp(c(\hat{\beta}_2 + \hat{\beta}_3x_1) \pm cZ_{1-a/2}\sqrt{\widehat{Var}(\hat{\beta}_2 + \hat{\beta}_3x_1)})
$$

$$
\large \widehat{Var}(\hat{\beta}_2 + \hat{\beta}_3x_1) = \widehat{Var}(\hat{\beta}_2) + x_1^2\widehat{Var}(\hat{\beta}_3) + 2x_1\widehat{Cov}(\hat{\beta}_2, \hat{\beta}_3)
$$

```{r}
#Is there an interaction between wind and distance? The longer the distance, the more the football is influenced from it, right?

mod.fit.Ho <- glm(formula = good ~ distance + wind, family = binomial(link = logit), data = placekick)
mod.fit.Ha <- glm(formula = good ~ distance + wind + distance:wind, family = binomial(link = logit), data = placekick)
summary(mod.fit.Ha)
```

After testing if $\beta_3$ is important to the model, both the Wald and LRT suggests that there is marginal evidence to support a wind and distance interaction. The code will not be found here.

```{r}
    par(mfrow = c(1,2))

    curve(expr = predict(object = mod.fit.Ho, newdata = data.frame(distance = x, wind = 0), type = "response"), col = "red", lty = "solid", xlim = c(20,60),
          ylim = c(0,1), ylab = "Estimated probability", main = "Without interaction",
          xlab = "Distance", panel.first = grid(col = "gray", lty = "dotted"), cex.main = 0.9, lwd = 1)
    curve(expr = predict(object = mod.fit.Ho, newdata = data.frame(distance = x, wind = 1), type = "response"),
      col = "blue", lty = "dotdash", lwd = 1, add = TRUE)
    legend(x = 20, y = 0.4, legend = c("Wind = 0", "Wind = 1"), lty = c("solid", "dotdash"), col = c("red", "blue"),
      lwd = c(1,1), bty = "n")

    curve(expr = predict(object = mod.fit.Ha, newdata = data.frame(distance = x, wind = 0), type = "response"), col = "red", lty = "solid", xlim = c(20,60),
          ylim = c(0,1), ylab = "Estimated probability", main = "With interaction",
          xlab = "Distance", panel.first = grid(col = "gray", lty = "dotted"), cex.main = 0.9, lwd = 1)
    curve(expr = predict(object = mod.fit.Ha, newdata = data.frame(distance = x, wind = 1), type = "response"),
      col = "blue", lty = "dotdash", lwd = 1, add = TRUE)
    legend(x = 20, y = 0.4, legend = c("Wind = 0", "Wind = 1"), lty = c("solid", "dotdash"), col = c("red", "blue"),
      lwd = c(1,1), bty = "n")
```

You can see that with interaction, estimated probability of success decreases much faster as distance decreases.

Because of the interaction in the model, we need to interpret the effect of wind at specific levels of distance and vice-versa.

#### Odds ratio estimates and 95% Wald Confidence Intervals:

```{r}
beta.hat <- mod.fit.Ha$coefficients[2:4] #vector of the beta estimates
c <- 1 #doing c = 1 because we want to just interpret the effect of wind at different levels of distance. Since wind is a binary variable, how do the odds change including distance as an interaction variable?
distance <- seq(from = 20, to = 60, by = 10)

OR.wind <- exp(c*(beta.hat[2] + beta.hat[3]*distance))
cov.mat <- vcov(mod.fit.Ha)[2:4, 2:4]

#Var(beta-hat_2 + distance*beta-hat_3)
var.log.OR <- cov.mat[2,2] + distance^2*cov.mat[3,3] + 2*distance*cov.mat[2,3]

ci.log.OR.low <- c*(beta.hat[2] + beta.hat[3] * distance) - c*qnorm(p = 0.975)*sqrt(var.log.OR)
ci.log.OR.up <- c*(beta.hat[2] + beta.hat[3]*distance) + c*qnorm(p = 0.975) * sqrt(var.log.OR)

round(data.frame(distance = distance, OR.hat = 1/OR.wind, OR.low = 1/exp(ci.log.OR.up), OR.up = 1/exp(ci.log.OR.low)), 2)
#We invert because a majority of the OR.wind results are under 1. We also exp() the CI cuz its part of the equation.
```

Odds increase with more sufficient evidence as distance increases. Intervals that do not include one indicate that wind does have an effect on the success or failure of a placekick at a distance.

**Odds ratio estimate with Profile LR:**

```{r}
install.packages('mcprofile')
library(package = mcprofile)

K <- matrix( data = c(0, 0, 1, 20,
                      0, 0, 1, 30,
                      0, 0, 1, 40,
                      0, 0, 1, 50,
                      0, 0, 1, 60), nrow = 5, ncol = 4, byrow = TRUE)
#K <- cbind(0, 0, 1, distance) # a quicker way to form K

linear.combo <- mcprofile(object = mod.fit.Ha, CM = K)
ci.log.OR <- confint(object = linear.combo, level = 0.95, adjust = 'none')

data.frame(distance, OR.hat = round(1/exp(ci.log.OR$estimate), 2), OR.low = round(1/exp(ci.log.OR$confint$upper),2), OR.up = round(1/exp(ci.log.OR$confint$lower), 2))
```

# Categorical Explanatory Variables

Let's say you have a column that only has the values A, B, C, D. Then you can represent them as factors and convert the values to indicator variables.

$$
\begin{align}
\large logit(\pi) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3
\end{align}
$$

is the same as

$$
\large logit(\pi) = \beta_0 + \beta_1B + \beta_2C + \beta_3D
$$

A is included in the model when all the other indicator parameters are equal to 0.

Also, when we test for a column that's been converted to a bunch of indicator variables, we have to set **every** indicator variable to 0 to test the column.

```{r}
tomato <- read.csv(file = './TomatoVirus.csv')
head(tomato)

tomato$Control <- as.factor(tomato$Control)
levels(tomato$Control)
contrasts(tomato$Control) 
```

```{r}
#Infest is numeric but still factor level. It doesn't matter since theres only 1 and 2, but if you had more you have to change it to factor class
contrasts(factor(tomato$Infest))
tomato$Infest <- factor(tomato$Infest)
```

```{r}
mod.fit <- glm(formula = Virus8/Plants ~ Infest + Control, family = binomial(link = logit), data = tomato, weights = Plants)
summary(mod.fit)
```

The estimated logistic regression model is:

$$
\large logit(\hat{\pi}) = -0.6652 + 0.2196Infest2 - 0.7933 ControlC+ 0.5152ControlN
$$

##### Interaction Variables

```{r}
#Model with interaction between Infest and Control
mod.fit.inter <- glm(formula = Virus8/Plants ~ Infest + Control + Infest:Control, family = binomial(link = logit), data = tomato, weights = Plants)
summary(mod.fit.inter)

#Testing all the interaction parameters
library(package = car)
Anova(mod.fit.inter)
```

The estimated logistic regression model is:

$$
\large \begin{align}
 logit(\hat{\pi}) = &-1.0460 + 0.9258Infest2 - 0.1623ControlC + 1.1260ControlN\\ &- 1.2114Infest2\times ControlC - 1.1662Infest2\times ControlN
\end{align}
$$

Remember, Anova() is a LRT test we use. We used it in this case to test if there is an interaction between the infestation and control methods.

$$
\large \begin{align}
H_0\::\:logit(\pi) = \begin{split}\beta_0 + \beta_1Infest2 + \beta_2ControlC + \beta_3ControlN\end{split}\\
H_a\::\:logit(\pi) = \begin{split}\beta_0 + \beta_1Infest2 + \beta_2ControlC + \beta_3ControlN \\+\beta_4Infest2 \times ControlC + \beta_5Infest2 \times ControlN
\end{split}
\end{align}
$$

### Odds Ratios for Indicator Variables

Model example:

$$
\large logit(\pi) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2
$$

The odds of success at level B is exp($\large \beta_0 + \beta_1$) because x_1 = 1, x_2 = 0, and x_3 = 0.

Thus, the resulting odds ratio of comparing level B to level C is:

$$
\large \frac{Odds_{x_1 = 1, x_2 = 0, x_3 = 0}}{Odds_{x_1 = 0, x_2 = 1, x_3 = 0}} = \frac{exp(\beta_0 +\beta_1)}{exp(\beta_0+\beta_2)} = exp(\beta_1 - \beta_2)
$$

with a CI of:

$$
\large exp\left(\hat{\beta_1}-\hat{\beta_2}\pm Z_{1-a}\sqrt{\widehat{Var}(\hat{\beta}_1 - \hat{\beta}_2)}\right)
$$

with Var:

$$
\large \widehat{Var}(\hat{\beta}_1 - \hat{\beta}_2) = \widehat{Var}(\hat{\beta}_1) +\widehat{Var}(\hat{\beta}_2) - 2\widehat{Cov}(\hat{\beta}_1, \hat{\beta}_2)
$$

### Odds Ratios for Interaction Variables

Imagine X and Z are three level variables. Then, for example, the comparison of levels B and C of X at level B of Z is:

$$
\large \frac{Odds_{x_1 = 1, x_2 = 0, z_1= 1, z_2 = 0}}{Odds_{x_1 = 0, x_2 = 1, z_1= 1, z_2 = 0}} = \frac{exp(\beta_0 +\beta_1+\beta_3 + \beta_5)}{exp(\beta_0+\beta_2+\beta_3 + \beta_7)} = exp(\beta_1 - \beta_2+\beta_5 - \beta_7)
$$

Where beta 5 and beta7 are interaction terms

```{r}
#Odds ratio of Control without interaction
exp(mod.fit$coefficients[3:4])

#Control N vs. Control C
exp(mod.fit$coefficients[4] - mod.fit$coefficients[3])
```

The odds ratio of no control vs biological control(base level) is 1.67. Inverted, we get 1/1.67 = 0.5973. This means that the estimated odds of using bio control vs no control is 0.59 times as large. This means that using the bio control reduces the odds of a plant showing symptoms by approximately 40%.

```{r}
#CI of odds comparing level N to level B
#Wald interval
exp(confint.default(object = mod.fit, parm = c("ControlC", "ControlN"), level = 0.95))

#Profile likelihood ratio interval
exp(confint(object = mod.fit, parm = c("ControlC", "ControlN"), level = 0.95))
```

The odds of plants showing symptoms with no treatment is 1.29 to 2.17 times larger when compared to using a biological control. Alternatively, we can invert these numbers and say that the odds of plants showing symptoms are between 0.46 and 0.77 times as large when suing bio control rather than no control. Thus, the bio control reduces odds of a plant showing symptoms by approximately 23% to 54%.

```{r}
#Wald interval for Control N vs. Control C
beta.hat <- mod.fit$coefficients[-1] #Matches up beta indices. Basically getting rid of the first index in the coef vector. Really weird.
exp(beta.hat[3] - beta.hat[2])

cov.mat<- vcov(mod.fit)[2:4, 2:4]
var.N.C <- cov.mat[3,3] + cov.mat[2,2] - 2*cov.mat[3,2]
CI.betas <- beta.hat[3] - beta.hat[2] + qnorm(p = c(0.025, 0.975))* sqrt(var.N.C)

exp(CI.betas)
```

### Odds ratio holding Infest2 constant

Comparing level N to level B

$$
\large \frac{Odds_{ControlC= 0, ControlN = 1, Infest2= 0}}{Odds_{
ControlC= 0, ControlN = 0, Infest2= 0
}} = \frac{exp(\beta_0 +\beta_1+\beta_3 + \beta_5)}{exp(\beta_0+\beta_1)} = exp(\beta_3+\beta_5)
$$

```{r}
#Estimated OR for Control holding Infest2 constant
beta.hat <- mod.fit.inter$coefficients[-1]
N.B.Infest2.0 <- exp(beta.hat[3])
N.B.Infest2.1 <- exp(beta.hat[3] + beta.hat[5])
C.B.Infest2.1 <- exp(beta.hat[2])
C.B.Infest2.0 <- exp(beta.hat[2] +beta.hat[4])
N.C.Infest2.0 <- exp(beta.hat[3] - beta.hat[2])
N.C.Infest2.1 <- exp(beta.hat[3] - beta.hat[2] + beta.hat[5] - beta.hat[4])

comparison <- c("N vs. B", "N vs. B", "C vs. B", "C vs. B", "N vs. C", "N vs. C")
data.frame(Infest = c(0,1,0,1,0,1), Control = comparison, OR.hat = round(c(N.B.Infest2.0, N.B.Infest2.1, C.B.Infest2.0, C.B.Infest2.1, N.C.Infest2.0, N.C.Infest2.1),2))
```

CI

```{r}
# Using mcprofile
    # library(package = mcprofile)  # If had not done yet
#Each row needed for odds ratio
    K<-matrix(data = c(0, 0,  0,  1,  0,  0,
                       0, 0,  0,  1,  0,  1,#comparing OR of level n to b with infest1
                       0, 0,  1,  0,  0,  0, 
                       0, 0,  1,  0,  1,  0,
                       0, 0, -1,  1,  0,  0,
                       0, 0, -1,  1, -1,  1),  nrow = 6, ncol = 6, byrow = TRUE)
    linear.combo<-mcprofile(object = mod.fit.inter, CM = K)
    ci.log.OR<-confint(object = linear.combo, level = 0.95, adjust = "none")
    ci.log.OR
    data.frame(Infest2 = c(0, 1, 0, 1, 0, 1), comparison, OR = round(exp(ci.log.OR$estimate),2),
       OR.CI = round(exp(ci.log.OR$confint),2))
    
#Wald way
    save.wald<-wald(object = linear.combo)
    ci.logit.wald<-confint(object = save.wald, level = 0.95, adjust = "none")
    data.frame(Infest2 = c(0, 1, 0, 1, 0, 1), comparison, OR = round(exp(ci.log.OR$estimate),2),
      lower = round(exp(ci.logit.wald$confint[,1]),2), upper = round(exp(ci.logit.wald$confint[,2]),2))
```

Although remember, since we are doing all of these OR at once, we need to recognize the overall familywise confidence level. There are two ways to do this: bonferroni and single-step

```{r}
#Bonferroni
ci.log.bon <- confint(object = linear.combo, level = 0.95, adjust = 'bonferroni')

#Single Step
ci.log.ss <- confint(object = linear.combo, level = 0.95, adjust = 'single-step')
data.frame(Infest2 = c(0, 1, 0, 1,0, 1), comparison, bon = round(exp(ci.log.bon$confint), 2), ss = round(exp(ci.log.ss$confint),2))
```

Both of these intervals allows us to claim that we are 95% confident that the process has covered ALL of the odds ratios, rather than merely having 95% confidence separately in each one. Same syntax for Wald

## Convergence of parameter estimation

glm() is used by itteratively changing the parameters until convergence is attained or until the maximum \# of iterations is reached. Criteria used to determine convergence is change in residual deviance. Thus, if G\^(k) is residual deviance at iteration k:

$$
\large \frac{|G^{(k)} - G^{(k-1)}|}{0.1 + |G^{(k)}|} < \epsilon
$$

You can pick your epsilon and maximum iterations in the procedure. Example:

```{r}
mod.fit <- glm(formula = good ~ distance, family = binomial(link = logit), data = placekick, trace = TRUE, epsilon = 0.0001, maxit = 50) #trace = TRUE shows the residual deviances per iteration)
mod.fit$control #convergence control values
```

Sample data set: x1 \<=5 means y = 0. if x1 \>5 then y = 1. Complete separation.

```{r}
set1 <- data.frame(x1 = c(1,2,3,4,5,6,7,8,9,10), y = c(0,0,0,0,0,1,1,1,1,1))
head(set1)
```

This doesn't work because the data is completely separated.

```{r}
mod.fit1 <- glm(formula = y ~ x1, data = set1, family = binomial(link = logit), trace = TRUE)
```

```{r}
mod.fit1$coefficients #last iteration parameters
```

There are two ways to deal with this.

1.  Exact logistic regression
2.  Modified likelihood function

we focus on 2.Basically:

$$
\large \sum^n_{i = 1}(y_i - \pi_i + h_i(0.5 - \pi_i))x_{ir}
$$

Where h is the ith diagonal element from the hat matrix, **H =** $V^{1/2}X(X'VX)X'V^{1/2}$. It is the penalty that ensures that the y\^ith isnt always 0 or 1 and brings it closer.

This equation is basically the first derivative with respect to $\large \beta_r$. Also known as the modified score function since we set each derivative equal to 0 and solve for the parameter estimate from there.

```{r}
install.packages('logistf')
library(package = logistf)
mod.fit.firth1 <- logistf(formula = y ~ x1, data = set1)
options(digits = 4) #Controls printing in R Console window
summary(mod.fit.firth1)

```

This is wald's way , but you can use logistftest() for LRT way. Also, functions like predict() and anova() are no longer available. Use names() to see items with the function.

## Generalized Linear Models

Basically, logistic regression is taking the inverse CDF of a linear function.
